---
title: Disclosure Risk for Student Microdata
author: Shaina Trevino
date: '2021-03-16'
slug: []
categories: []
tags: []
subtitle: ''
excerpt: 'This blog post walks through one example of reducing disclosure risk for student microdata under FERPA. Specifically, assessing re-identification risk and applying statistical disclosure control for a simulated, student data set with the sdcMicro R package'
images: ~
series: ~
layout: single
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(scales)
library(sdcMicro)
```

# Overview

Under the Family Educational Rights and Privacy Act (FERPA), schools can release data to researchers without parent or student consent if the data is released under a [FERPA study exception](https://studentprivacy.ed.gov/sites/default/files/resource_document/file/FERPA%20Exceptions_HANDOUT_portrait.pdf) with a data sharing agreement or if the data has been properly de-identified. Proper data de-identification involves removing or altering all personally identifiable information (PII) from the data to minimize the disclosure risk of individuals. It is important to note that PII includes not only the obvious identifiable information about individuals (e.g., direct identifiers; names, student IDs, social security number, date of birth, address), but also any other information that can be used to identify an individual alone or in combination with other information that can be linked to the data (i.e., indirect identifiers). Thus, simply removing direct identifiers does not properly de-identify the data. Careful attention must be payed to the identification and altering of indirect identifiers to adequately reduce disclosure risk. 

A disclosure occurs when an intruder can reveal the identity of an individual in the data set or learn confidential information about them. Thus, measuring disclosure risk involves calculating risk metrics related to an intruder's ability to identify individuals (i.e., re-identification risk). After assessing the re-identification risk of the original data, statistical disclosure control techniques can be applied to reduce disclosure risk to an appropriate level. There are no specific guidelines or methods to properly de-identify data because the re-identification risk varies depending on the structure, context, and purpose of the data. There may also be other federal, state, or local laws that require different levels of de-identification. 

For our purposes, we will be walking through the process of statistical disclosure control under FERPA with cross-sectional, student-level micro-data (i.e., one student per row). However, it is important to note that there are certain characteristics of micro-data files that can increase the likelihood of a disclosure (we only need to attend to the first): 

+ Existence of rare or detailed records (e.g., unique characteristics or extreme values)

+ Multilevel structure (i.e., if one level is too detailed, disclosure at other levels could be possible)

+ Linking to external sources (e.g., rare combinations of variables, shared linking keys)

+ Longitudinal structure (i.e., adding records over time could increase disclosure risk)

+ Census data (i.e., if an intruder knows who is in the data, disclosure is more likely) 

Specifically, this blog will explain the basic steps of assessing disclosure risk and applying statistical disclosure control techniques with the `{sdcMicro}` package and briefly mention how to re-assess disclosure risk and data utility after transformation when releasing FERPA-compliant student micro-data. Code and data available on [GitHub](LINKREPO)

## Data Description

This data set was simulated to represent the distributions of administrative student variables that a researcher would potentially request access to. Thus, distributions and descriptive statistics represent a student population, however, relationships (e.g., correlations) among variables are not representative, nor realistic, and should not be interpreted. 

When possible, the variables selected for de-identification and release should be limited to those necessary for data analysis in order to minimize disclosure risk and maximize data utility. Our data set includes a sample of 25,000 students and 13 variables including: 

+ `id`: study-specific randomized ID number for each student

+ `grade_level`: student's grade level

+ `sex`: student sex

+ `race`: student race/ethnicity

+ `gpa`: student's unweighted grade point averages 

+ `econ_dis`: student flagged as economically disadvantaged

+ `disability`: student flagged with a disability (i.e., individual education plan)

+ `dis_cat`: student disability type

+ `lang`: student first language

+ `iss_off`: count of behavior incidents that resulted in in-school suspension

+ `iss_days`: total number of days served in in-school suspension

+ `subj_off`: count of behavior incidents that were classified as subjective (i.e., subjective to interpretation) 

+ `days_absent`: total number of days absent

We will use the `import()` and `here()` function to import our data and tidy it with the `{tidyverse}` package (e.g., modifying character variables to factors and defining missing values). It is important to verify that all variables are classified correctly (e.g., numeric or factor) and missing values are defined as `NA`.  


```{r import}
sim_df <- rio::import(here::here("data", "sim_df.csv")) %>% #import data
  mutate_if(is.character, as.factor) %>% #transform characters to factors
  mutate_all(na_if, "") #transform blank cells to missing

#use str() to view variables and types
str(sim_df)
```

# Preprocessing

Before beginning the disclosure risk assessment, some decisions need to be made about the data and types of risks to be protecting against. Direct identifiers should also be removed or obscured from the data in this step as disclosure risk assessment and control procedures are concerned with protecting indirect identifiers. 

## Understand the Data and Context

It can be very helpful to explore the variables and relations among variables in your data set. Looking at the distributions will give a sense of which variables have rare values and are more risky. It will be necessary to fully understand the structure and distributions of variables when making decisions for applying appropriate statistical disclosure techniques. 

It is also crucial to understand the context in which the data is going to be released as this can affect the de-identification process. For example, data shared under a strict data sharing agreement may need less protection and can have a higher acceptable disclosure risk compared to data that is shared without such agreements or will be widely shared. 

Understanding the data and data context makes it possible to develop what are called "motivated intruder scenarios." These intruder scenarios guide the types of protections that need to be applied to the data to appropriately reduce disclosure risk. To develop an intruder scenario, document the ways in which an individual in your data can be identified by an intruder. For example, our data includes many demographic variables, therefore an intruder could identify someone in our data set if they linked unique demographic variables with public registries that include identifiable information (e.g., voter registration, DMV records). Another scenario that is almost always present is the possibility of an inadvertent disclosure which can occur when there are unique combinations of demographic variables resulting in an accidental disclosure. Developing intruder scenarios can sometimes require the support of a subject matter expert to determine which auxiliary data sets are publicly available and contain similar information to the data set to be de-identified. 

## Selecting Key Variables (PII)

Arguably,  one of the most challenging parts of proper data de-identification under FERPA is the classification indirect identifiers, also called key variables in the data de-identification literature or PII in FERPA. FERPA defines indirect identifiers as including "information that, alone or in combination, is linked or linkable to a specific student that would allow a reasonable person in the school community, who does not have personal knowledge of the relevant circumstances, to identify the student with reasonable certainty" ([FERPA](https://www2.ed.gov/policy/gen/guid/fpco/pdf/ferparegs.pdf), p. 6). This vague definition is the reason why classifying indirect identifiers can be so difficult. 

Classifying variables as indirect identifiers or non-identifiers should include consideration of intruder scenarios, conceptual understanding of variables and potential linkages, and exploring variable risk alone and in combination with one another (e.g., rare values, small cell sizes). It can be helpful to assess disclosure risk multiple times with different sets of indirect identifiers or intruder scenarios to understand how different variables affect disclosure risk (e.g., higher re-identification risk, increased suppression) and may be indirect identifiers. For our purposes, we will assume that all variables can be linked with a specific student when combined with other variables in the data set (e.g., only one Asian female student with disability that speaks German). This is a very conservative approach that defines all "real" PII under FERPA correctly, but may classify non-PII variables as indirect identifiers and thus decrease the utility of the transformed data more than necessary. Often, data utility will decrease drastically the more number of indirect identifiers you define, however, more privacy protection is applied.  

# Disclosure Risk Assessment

Once you understand your data, the environment for release, and the variables that are indirect identifiers, you can begin the process of assessing disclosure risk in your data set. To do this we will use the `{sdcMicro}` package and some functions from `{tidyverse}`. 

## Set up the `{sdcMicro}` Object

The first step of disclosure risk analysis in `{sdcMicro}` is to create an object that tells `R` which continuous and categorical variables should be considered indirect identifiers in your data set. This is implemented with the `createSdcObj()` function (as shown below) which results in a `{sdcMicro}` object with many different layers (e.g., original data, transformed data, risk/utility metrics, and many other configuration settings). Mostly everything that we do to assess risk and de-identify data will involve manipulating this object. For this function, the `keyVars` refer to the categorical indirect identifiers and `numVars` are the continuous indirect identifiers. 

```{r sdc-obj}
#create object
initial_sdcobj <- createSdcObj(dat = sim_df, #input data
                               keyVars = c("grade_level", "sex", "race", "econ_dis", 
                                           "disability", "dis_cat", "lang"), #categorical
                               numVars = c("gpa", "iss_off", "iss_days", 
                                           "subj_off", "days_absent")) #continuous 

```

For our `{sdcMicro}` object, we specified our original data, the categorical indirect identifiers, and the continuous indirect identifiers. There are many other arguments that you can specify if needed (e.g., sampling weights, PRAM values) - just see the help documentation. 

## Categorical Variables and *k*-anonymity

The methods for disclosure risk assessment and control differ for continuous vs. categorical variables (this is why they are defined separately in the object above). For categorical variables, disclosure risk can be assessed by calculating cross-tab frequency counts and the probability that an individual is unique or can be easily identified in the data set (e.g., small cell sizes). For this blog we will use the concept of *k*-anonymity to assess and control for disclosure risk of categorical variables. 

*k*-anonymity is a traditional privacy model that can be used to reduce re-identification risk. To achieve *k*-anonymity there needs to be at least *k* individuals with the same responses on all indirect identifiers for every combination of values in the indirect identifiers. The value of *k* should be specified in advance depending on a threshold for acceptable re-identification risk. For example, *k* = 2 means that there are no unique individuals in the data set since everyone will at least have 1 other individual with the same responses on all indirect identifiers and will not be distinguishable. However, the highest re-identification risk is still 50% since intruders have a 50/50 chance of identifying the correct individual when *k* = 2. 

Alternatively, 5-anonymity will require at least 5 individuals to have the same responses on all variables and will have a highest re-identification risk of 20%. There are no specific guidelines for which *k* threshold to us. Statisticians have recommended a minimum *k* value of at least 3-5 for privacy preserving data publishing but determining the exact threshold should take into account local or state laws as well as the data environment for release. It may be necessary to select a higher threshold if the data will be publicly shared or archived (e.g., *k* = 10+). 

*k*-anonymity is best suited when assessing and controlling for population uniqueness. It can be used for sample uniqueness, but will often overestimate the re-identification risk unless sampling weights are included in the de-identification process. For a more detailed definition of sample vs. population uniqueness see [International Household Survey Network](http://www.ihsn.org/anonymization-risk-measure). There were no sampling weights included in the `sdcMicro` object we created which means we are controlling for sample uniqueness assuming our sample of the student population. We will be using *k* = 5 for our threshold to release de-identified student data to a set of approved researchers under a data sharing agreement. 

### *k*-anonymity Violations

One simple way to assess disclosure risk of categorical indirect identifiers is to calculate the number of observations in the data set that violate *k*-anonymity. In other words, the number of individuals with unique or risky (i.e., small cell size) responses. To view the number of individuals that violate *k*-anonymity for *k* = 2, 3, and 5, use `print()` on the `{sdcMicro}` object we created above:

```{r k-anon-vio-sdcobj}
print(initial_sdcobj)
```

The above output shows that about 5% of students are unique in the sample (i.e., violate 2-anonymity) and 13% of students are below our specified re-identification risk threshold (5-anonymity). To calculate the number of individuals violating *k*-anonymity at higher thresholds we can use the `kAnon_violations()` function: 

```{r kten, results = FALSE}

kvio <- kAnon_violations(initial_sdcobj, #sdcMicro object
                         weighted = FALSE, #no sampling weights
                         k = 10) #k value threshold
print(kvio)

```
```{r kten-render, echo = FALSE}
kvio[1]
```

The above shows the number of students violating 10-anonymity is `r kvio[1]` (`r round(kvio[1] / nrow(sim_df), 4) * 100`%).

### Global Risk

Global risk refers to the average re-identification risk of the entire data set. It is calculated by aggregating all individual risk scores (described below). When we first created our `{sdcMicro}` object, global and individual risk metrics were calculated and stored within the `{sdcMicro}` object. To access it we `print()` the `"risk"` metric of our `{sdcMicro}` object:

```{r global-risk-OLD}
print(initial_sdcobj, "risk")
```

The global risk is `r round(initial_sdcobj@originalRisk$global$risk, 4)`, indicating that a motivated intruder could potentially identify `r round(initial_sdcobj@originalRisk$global$risk_ER)` (`r round(initial_sdcobj@originalRisk$global$risk, 4) * 100`%) of students , on average. This risk is very high, but is likely overestimated since we are not using sampling weights and thus assumes the intruder knows which individuals are included in the sample. The output also shows that 5395 students have individual risk metrics that are operating differently compared to the majority of individual risk metrics (e.g., lower or higher). I find this measure less helpful, but it is interesting that it is the same as the 10-anonymity violations we calculated. After applying statistical disclosure control methods, we expect to see the global risk metrics reduce - especially the expected number of re-identifications. 

It is important to note that global risks should be not be given too much weight when assessing disclosure risk. Although global risk metrics are easy to compute, they are aggregate measures that don't take into account unique individuals in the data set. It is possible to have an acceptable global risk but still have unique or high risk individuals in the data set. Thus, investigating individual risk is important when assessing disclosure risk. We did this briefly when calculating *k*-anonymity violations, but will want to look at individual risk in more depth next. 

### Individual Risk and Sample Frequencies/Uniqueness

Individual risk is directly related to *k*-anonymity and refers to the probability of correctly re-identifying each individual student. As mentioned above, an individual that is 2-anonymous (i.e., in an equivalence class of 2) will have an individual risk of .5 indicating an intruder has a 50% chance of identifying this student successfully, whereas a unique individual will have an individual risk of 1 (100%). Alternatively, when an individual has the same responses as many others in the data set, they will all have a lower individual re-identification risk. 

Individual risk metrics were already calculated and stored when we created the initial `{sdcMicro}` object. To view the individual risk for each observation, we extract the risk metrics and frequency counts (i.e., number of individuals who have the same responses on all indirect identifiers) and combine them with the original data set. 


```{r samp-freq-risk, results = FALSE}
ind_risk <- data.frame(initial_sdcobj@risk$individual) %>% #extract risk metrics
  select(-Fk) #de-select population metric

ind_risk_df <- cbind(sim_df, ind_risk) #combine with data

head(ind_risk_df, 10)

```

```{r samp-freq-risk-render, echo = FALSE}
ind_risk_df %>% 
  select(id, where(is.factor), risk, fk) %>% 
  head(10) %>% 
  as_tibble()
```

This will give you a data frame that includes the individual risk metrics (`risk`) and frequency counts for that combination for responses (`fk`). For example, we can see the 3rd observation is unique (i.e., only one person has this combination of responses in the data set) and has a 100% individual re-identification risk, whereas the 1st observation shares the same responses with 198 other individuals resulting in low re-identification risk (0.5%).

It is important to note that missing values will affect the calculation of individual risk and frequency counts (`fk`). If there are missing values in a combination, the frequency count will include the count of all possible responses for the missing value. For example, the response [10th Grade, Female, White] may have a combined frequency (`fk`) of 198, and the response [10th Grade, Male, White...] `fk` = 189. However an individual a missing value for `sex` [10th Grade, NA, White...] would have a combined frequency of `r 198+189` (198 + 189) and a much higher individual risk metric. Thus, it is very important to make sure that missing values are not identifiable.

Observing the individual risk metrics for each student in the data can be helpful to see which individuals have unique responses (i.e., filter when `fk` = 1) or are under a certain threshold (e.g., 5-anonymity). However, it is often more useful to look at sample frequencies and risk for each combination of responses possible instead of for each student. 


#### Risky Variable Response Combinations

We will use various functions from the `{tidyverse}` package to aggregate the individual risk metrics for each variable response combination:

```{r agg-freq-keys}
var_combos <- ind_risk_df %>% 
  select(id, where(is.factor), risk, fk) %>% #select variables
  group_by(grade_level, sex, race, econ_dis, disability, dis_cat, lang) %>%  #group by all responses
  summarize(risk = mean(risk), #aggregate risk metrics
            freq = mean(fk),
            .groups = "keep")

var_combos

```
The resulting data frame shows the `r comma(nrow(var_combos))` possible response combinations for our categorical indirect identifiers and the corresponding risk metrics. You can see that the first response combination is unique, whereas the second combination has 11 students with those responses. 

Next, we can filter each combination to show us only the unique response combinations: 

```{r agg-uniq}
uniq <- var_combos %>% 
  filter(freq == 1)

uniq
```

We already know from calculating the *k*-anonymity violations above that there are `r comma(nrow(uniq))` unique individual responses in our data set. However, it is helpful to explore the pattern of unique responses to see which variables or response options are leading to high re-identification risk. 

It is useful to also filter variable response combinations for those that are under our threshold. Since we are using 5-anonymity as our privacy model, we filter for response combinations with less than 5 students: 

```{r agg-risk}
#filter high risk
risky <- var_combos %>% 
  filter(freq < 5)

risky
```

The resulting data frame shows the risk metrics for the `r comma(nrow(risky))` possible response combinations that are under our threshold. Similar to the above step, it can be helpful here to explore this data frame for patterns of risky responses to see if there are certain variables or response options that are most often leading to risky variable combinations. 

## Continuous Variables

The concept of *k*-anonymity and uniqueness often does not apply when measuring risk of continuous variables since continuous variables can have infinite values and would make most individuals unique when looking at frequencies of combined variables responses. For continuous variables, disclosure risk is often measured with probabilistic or distance-based metrics and by exploring outliers. While exploring distributions and outliers can give you a sense of the amount of risk (e.g., skewness) in continuous variables, probabilistic and distance-based risk metrics have to be calculated after applying statistical disclosure control techniques because they compare the transformed data to the raw data to assess re-identification risk. 

Two distance-based risk metrics are calculate when using `{sdcMicro}`: record linkage and interval measures. Both metrics are based on similarity or uniqueness among transformed values and how close those values are to the original values (e.g., in the same neighborhood of values). Thus, these measures are most useful when using statistical disclosure techniques that alter the true data values (i.e., perturbative methods), such as adding statistical noise to variables. As discussed in later sections, we will not be using perturbative methods which makes these distance-based metrics less useful in our case. The calculation of these risk metrics are discussed in later sections after applying disclosure control methods. 

To assess risk of our continuous variables before applying de-identification techniques, we will explore the distributions of our continuous variables and look for rare values. 


#### Explore distributions

To plot basic frequency distributions of our continuous variables we can use the `hist()` function for each continuous variable in our original data set (`sim_df`), such as for `gpa`:

```{r gpa-code, eval = FALSE}
#plot for gpa from original data
hist(sim_df$gpa)
```

```{r gpa-render, echo = FALSE}
hist(sim_df$gpa,
     main = "Histogram of gpa",
     xlab = "gpa")
```






