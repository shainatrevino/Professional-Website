---
title: "Disclosure Risk for Student Microdata"
subtitle: ""
excerpt: "This blog post walks through one example of reducing disclosure risk for student microdata under FERPA. Specifically, assessing re-identification risk and applying statistical disclosure control for a simulated, student data set with the sdcMicro R package"
date: 2021-03-16
author: "Shaina Trevino"
draft: false
images:
tags: 
  - dataprivacy
categories:
layout: single
---



# Overview

Under the Family Educational Rights and Privacy Act (FERPA), schools can release data to researchers without parent or student consent if the data is released under a [FERPA study exception](https://studentprivacy.ed.gov/sites/default/files/resource_document/file/FERPA%20Exceptions_HANDOUT_portrait.pdf) with a data sharing agreement or if the data has been properly de-identified. Proper data de-identification involves removing or altering all personally identifiable information (PII) from the data to minimize the disclosure risk of individuals. It is important to note that PII includes not only the obvious identifiable information about individuals (e.g., direct identifiers; names, student IDs, social security number, date of birth, address), but also any other information that can be used to identify an individual alone or in combination with other information that can be linked to the data (i.e., indirect identifiers). Thus, simply removing direct identifiers does not properly de-identify the data. Careful attention must be payed to the identification and altering of indirect identifiers to adequately reduce disclosure risk. 

A disclosure occurs when an intruder can reveal the identity of an individual in the data set or learn confidential information about them. Thus, measuring disclosure risk involves calculating risk metrics related to an intruder's ability to identify individuals (i.e., re-identification risk). After assessing the re-identification risk of the original data, statistical disclosure control techniques can be applied to reduce disclosure risk to an appropriate level. There are no specific guidelines or methods to properly de-identify data because the re-identification risk varies depending on the structure, context, and purpose of the data. There may also be other federal, state, or local laws that require different levels of de-identification. 

For our purposes, we will be walking through the process of statistical disclosure control under FERPA with cross-sectional, student-level micro-data (i.e., one student per row). However, it is important to note that there are certain characteristics of micro-data files that can increase the likelihood of a disclosure (we only need to attend to the first): 

+ Existence of rare or detailed records (e.g., unique characteristics or extreme values)

+ Multilevel structure (i.e., if one level is too detailed, disclosure at other levels could be possible)

+ Linking to external sources (e.g., rare combinations of variables, shared linking keys)

+ Longitudinal structure (i.e., adding records over time could increase disclosure risk)

+ Census data (i.e., if an intruder knows who is in the data, disclosure is more likely) 

Specifically, this blog will explain the basic steps of assessing disclosure risk and applying statistical disclosure control techniques with the `{sdcMicro}` package and briefly mention how to re-assess disclosure risk and data utility after transformation when releasing FERPA-compliant student micro-data. Code and data available on [GitHub](https://github.com/shainatrevino/Professional-Website)

## Data Description

This data set was simulated to represent the distributions of administrative student variables that a researcher would potentially request access to. Thus, distributions and descriptive statistics represent a student population, however, relationships (e.g., correlations) among variables are not representative, nor realistic, and should not be interpreted. 

When possible, the variables selected for de-identification and release should be limited to those necessary for data analysis in order to minimize disclosure risk and maximize data utility. Our data set includes a sample of 25,000 students and 13 variables including: 

+ `id`: study-specific randomized ID number for each student

+ `grade_level`: student's grade level

+ `sex`: student sex

+ `race`: student race/ethnicity

+ `gpa`: student's unweighted grade point averages 

+ `econ_dis`: student flagged as economically disadvantaged

+ `disability`: student flagged with a disability (i.e., individual education plan)

+ `dis_cat`: student disability type

+ `lang`: student first language

+ `iss_off`: count of behavior incidents that resulted in in-school suspension

+ `iss_days`: total number of days served in in-school suspension

+ `subj_off`: count of behavior incidents that were classified as subjective (i.e., subjective to interpretation) 

+ `days_absent`: total number of days absent

We will use the `import()` and `here()` function to import our data and tidy it with the `{tidyverse}` package (e.g., modifying character variables to factors and defining missing values). It is important to verify that all variables are classified correctly (e.g., numeric or factor) and missing values are defined as `NA`.  

























































































































